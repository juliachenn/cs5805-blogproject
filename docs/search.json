[
  {
    "objectID": "posts/5-anomaly-outlier-detection/index.html",
    "href": "posts/5-anomaly-outlier-detection/index.html",
    "title": "Anomaly/Outlier Detection: Country Data",
    "section": "",
    "text": "In this blog post, I will demonstate anomaly/outlier detection using country data (e.g. information on health, economics, etc). I’ll be using DBSCAN to find anomalies that lie outside clusters."
  },
  {
    "objectID": "posts/5-anomaly-outlier-detection/index.html#preparing-the-data",
    "href": "posts/5-anomaly-outlier-detection/index.html#preparing-the-data",
    "title": "Anomaly/Outlier Detection: Country Data",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nFirst, let’s read in the country data from the csv file.\n\nimport pandas as pd\n\ndataset = pd.read_csv(\"country_data/Country-data.csv\")\ndataset\n\n\n\n\n\n\n\n\ncountry\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\n0\nAfghanistan\n90.2\n10.0\n7.58\n44.9\n1610\n9.44\n56.2\n5.82\n553\n\n\n1\nAlbania\n16.6\n28.0\n6.55\n48.6\n9930\n4.49\n76.3\n1.65\n4090\n\n\n2\nAlgeria\n27.3\n38.4\n4.17\n31.4\n12900\n16.10\n76.5\n2.89\n4460\n\n\n3\nAngola\n119.0\n62.3\n2.85\n42.9\n5900\n22.40\n60.1\n6.16\n3530\n\n\n4\nAntigua and Barbuda\n10.3\n45.5\n6.03\n58.9\n19100\n1.44\n76.8\n2.13\n12200\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n162\nVanuatu\n29.2\n46.6\n5.25\n52.7\n2950\n2.62\n63.0\n3.50\n2970\n\n\n163\nVenezuela\n17.1\n28.5\n4.91\n17.6\n16500\n45.90\n75.4\n2.47\n13500\n\n\n164\nVietnam\n23.3\n72.0\n6.84\n80.2\n4490\n12.10\n73.1\n1.95\n1310\n\n\n165\nYemen\n56.3\n30.0\n5.18\n34.4\n4480\n23.60\n67.5\n4.67\n1310\n\n\n166\nZambia\n83.1\n37.0\n5.89\n30.9\n3280\n14.00\n52.0\n5.40\n1460\n\n\n\n\n167 rows × 10 columns\n\n\n\nHere are the descriptions of each field.\n\npd.read_csv(\"country_data/data-dictionary.csv\")\n\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\n0\ncountry\nName of the country\n\n\n1\nchild_mort\nDeath of children under 5 years of age per 100...\n\n\n2\nexports\nExports of goods and services per capita. Give...\n\n\n3\nhealth\nTotal health spending per capita. Given as %ag...\n\n\n4\nimports\nImports of goods and services per capita. Give...\n\n\n5\nIncome\nNet income per person\n\n\n6\nInflation\nThe measurement of the annual growth rate of t...\n\n\n7\nlife_expec\nThe average number of years a new born child w...\n\n\n8\ntotal_fer\nThe number of children that would be born to e...\n\n\n9\ngdpp\nThe GDP per capita. Calculated as the Total GD...\n\n\n\n\n\n\n\nAnd more information on our country dataset:\n\ndataset.columns\n\nIndex(['country', 'child_mort', 'exports', 'health', 'imports', 'income',\n       'inflation', 'life_expec', 'total_fer', 'gdpp'],\n      dtype='object')\n\n\n\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 167 entries, 0 to 166\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   country     167 non-null    object \n 1   child_mort  167 non-null    float64\n 2   exports     167 non-null    float64\n 3   health      167 non-null    float64\n 4   imports     167 non-null    float64\n 5   income      167 non-null    int64  \n 6   inflation   167 non-null    float64\n 7   life_expec  167 non-null    float64\n 8   total_fer   167 non-null    float64\n 9   gdpp        167 non-null    int64  \ndtypes: float64(7), int64(2), object(1)\nmemory usage: 13.2+ KB\n\n\n\ndataset.describe()\n\n\n\n\n\n\n\n\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\ncount\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n\n\nmean\n38.270060\n41.108976\n6.815689\n46.890215\n17144.688623\n7.781832\n70.555689\n2.947964\n12964.155689\n\n\nstd\n40.328931\n27.412010\n2.746837\n24.209589\n19278.067698\n10.570704\n8.893172\n1.513848\n18328.704809\n\n\nmin\n2.600000\n0.109000\n1.810000\n0.065900\n609.000000\n-4.210000\n32.100000\n1.150000\n231.000000\n\n\n25%\n8.250000\n23.800000\n4.920000\n30.200000\n3355.000000\n1.810000\n65.300000\n1.795000\n1330.000000\n\n\n50%\n19.300000\n35.000000\n6.320000\n43.300000\n9960.000000\n5.390000\n73.100000\n2.410000\n4660.000000\n\n\n75%\n62.100000\n51.350000\n8.600000\n58.750000\n22800.000000\n10.750000\n76.800000\n3.880000\n14050.000000\n\n\nmax\n208.000000\n200.000000\n17.900000\n174.000000\n125000.000000\n104.000000\n82.800000\n7.490000\n105000.000000"
  },
  {
    "objectID": "posts/5-anomaly-outlier-detection/index.html#using-dbscan-to-find-anomalies",
    "href": "posts/5-anomaly-outlier-detection/index.html#using-dbscan-to-find-anomalies",
    "title": "Anomaly/Outlier Detection: Country Data",
    "section": "Using DBSCAN to Find Anomalies",
    "text": "Using DBSCAN to Find Anomalies\nLet’s import some libraries that we’ll need:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\nLet’s use some of the fields as our x values and scale them.\n\nx_data = dataset[['total_fer', 'imports', 'gdpp']]\nx_data = StandardScaler().fit_transform(x_data)\n\nx_data\n\narray([[ 1.90288227e+00, -8.24549559e-02, -6.79179609e-01],\n       [-8.59972814e-01,  7.08366942e-02, -4.85623236e-01],\n       [-3.84043958e-02, -6.41762328e-01, -4.65375609e-01],\n       [ 2.12815103e+00, -1.65315307e-01, -5.16268293e-01],\n       [-5.41946330e-01,  4.97567504e-01, -4.18171341e-02],\n       [-3.82933087e-01, -1.27978703e+00, -1.45791437e-01],\n       [-8.33470607e-01, -6.58828857e-02, -5.33232522e-01],\n       [-6.74457365e-01, -1.07677917e+00,  2.13069856e+00],\n       [-9.99109401e-01,  3.76925537e-02,  1.85708197e+00],\n       [-6.81082917e-01, -1.08506521e+00, -3.89857431e-01],\n       [-7.20836227e-01, -1.32171167e-01,  8.22811276e-01],\n       [-5.22069674e-01,  1.66126098e-01,  4.23331061e-01],\n       [-4.09435294e-01, -1.03949202e+00, -6.67961329e-01],\n       [-7.73840641e-01,  7.49797118e-02,  1.66131471e-01],\n       [-9.65981642e-01,  7.29576488e-01, -3.79460001e-01],\n       [-7.20836227e-01,  1.15216428e+00,  1.72027368e+00],\n       [-1.57664328e-01,  4.39565258e-01, -4.71942407e-01],\n       [ 1.59810689e+00, -4.01467309e-01, -6.67961329e-01],\n       [-3.76307536e-01,  9.86443578e-01, -5.90144772e-01],\n       [ 1.66987709e-01, -5.21614819e-01, -6.01089435e-01],\n       [-1.08524157e+00,  1.82698169e-01, -4.57167111e-01],\n       [-4.50299476e-02,  1.82698169e-01, -3.61948539e-01],\n       [-7.60589538e-01, -1.45379377e+00, -9.65404512e-02],\n       [-7.34087331e-01, -7.82624926e-01,  1.22229149e+00],\n       [-9.12977228e-01,  2.53129467e-01, -3.35134114e-01],\n       [ 1.93601003e+00, -7.16336645e-01, -6.77975696e-01],\n       [ 2.19440655e+00, -3.18606958e-01, -6.96800517e-01],\n       [-4.50299476e-02,  5.22425610e-01, -6.66429076e-01],\n       [ 1.43246810e+00, -8.24055101e-01, -6.37754057e-01],\n       [-8.73223918e-01, -6.58334399e-01,  1.88444363e+00],\n       [-1.84166535e-01,  6.17715014e-01, -5.28307423e-01],\n       [ 1.49872361e+00, -8.44770189e-01, -6.85035003e-01],\n       [ 2.41304976e+00, -1.40457202e-01, -6.60354787e-01],\n       [-7.07585124e-01, -6.45905346e-01, -3.51081209e-03],\n       [-8.99726125e-01, -1.00634787e+00, -4.59903277e-01],\n       [-6.21452951e-01, -1.20521272e+00, -3.67420871e-01],\n       [ 1.19394823e+00,  1.99270239e-01, -6.67359372e-01],\n       [ 2.37992200e+00,  1.12266870e-01, -6.91164015e-01],\n       [ 1.32645927e+00,  3.23560766e-01, -5.59499714e-01],\n       [-6.81082917e-01, -4.92613696e-01, -2.60710403e-01],\n       [ 1.53847692e+00, -1.48743237e-01, -6.42679156e-01],\n       [-9.26228332e-01, -3.64180151e-01,  2.93231782e-02],\n       [-1.01236050e+00,  4.39565258e-01,  9.76036564e-01],\n       [-9.52730539e-01,  6.63288207e-01,  3.74080076e-01],\n       [-7.14210675e-01, -1.36314184e-01,  2.46451079e+00],\n       [-2.30545397e-01, -5.63044994e-01, -4.11199525e-01],\n       [-1.90792086e-01, -6.00332153e-01, -4.54430945e-01],\n       [ 1.60362157e-01, -8.40627172e-01, -5.67160978e-01],\n       [-4.49188605e-01, -1.20236572e-02, -5.45818885e-01],\n       [ 1.49872361e+00,  4.97567504e-01,  2.26327120e-01],\n       [ 1.10119051e+00, -9.77346752e-01, -6.83064964e-01],\n       [-8.13593952e-01,  9.03583226e-01,  8.95188270e-02],\n       [-1.84166535e-01,  7.04718383e-01, -5.09701495e-01],\n       [-7.14210675e-01, -3.93181274e-01,  1.81877565e+00],\n       [-6.08201847e-01, -7.78481908e-01,  1.51232507e+00],\n       [ 7.50036264e-01, -1.15963952e+00, -2.30612578e-01],\n       [ 1.83000120e+00, -1.73601343e-01, -6.78687099e-01],\n       [-6.81082917e-01,  2.44843432e-01, -5.47460584e-01],\n       [-1.03223716e+00, -4.05610327e-01,  1.57799305e+00],\n       [ 8.75921747e-01, -4.10247802e-02, -6.37754057e-01],\n       [-9.72607194e-01, -6.70763451e-01,  7.62615628e-01],\n       [-4.69065260e-01,  9.56947997e-02, -3.06130756e-01],\n       [ 2.86247640e-01, -4.38754467e-01, -5.54574615e-01],\n       [ 1.58485579e+00, -1.52886255e-01, -6.73980893e-01],\n       [ 1.39271478e+00, -4.84327661e-01, -6.79507948e-01],\n       [-1.97417638e-01,  1.33445705e+00, -5.43082719e-01],\n       [ 2.53119882e-01,  7.37862523e-01, -6.73214767e-01],\n       [-1.12499488e+00,  1.22673860e+00,  7.43385133e-03],\n       [-4.95567467e-01, -1.48743237e-01,  1.58346538e+00],\n       [-2.30545397e-01, -8.19912084e-01, -6.35565125e-01],\n       [-3.10052018e-01, -1.01463391e+00, -5.39252087e-01],\n       [-7.87091745e-01, -1.13892444e+00, -3.52098342e-01],\n       [ 1.06806275e+00, -5.29900854e-01, -4.63186676e-01],\n       [-5.94950744e-01,  1.64104035e+00,  1.95558394e+00],\n       [ 5.43533288e-02, -5.79617065e-01,  9.65091901e-01],\n       [-9.85858298e-01, -8.15769066e-01,  1.24965315e+00],\n       [-5.15444123e-01,  1.12266870e-01, -4.53336479e-01],\n       [-1.03223716e+00, -1.37921946e+00,  1.72574601e+00],\n       [ 4.71763090e-01,  9.16012279e-01, -5.08059796e-01],\n       [-2.30545397e-01, -7.03907592e-01, -2.13101117e-01],\n       [ 9.42177265e-01, -5.50615942e-01, -6.56524155e-01],\n       [ 5.91023021e-01,  1.36760119e+00, -6.27903860e-01],\n       [-4.88941916e-01, -6.83192504e-01,  1.39740611e+00],\n       [ 1.00732191e-01,  1.44217551e+00, -6.61285084e-01],\n       [ 1.33859950e-01,  9.98378173e-02, -6.47057021e-01],\n       [-1.05211382e+00,  3.40132836e-01, -9.10681195e-02],\n       [-8.86475021e-01,  5.51426733e-01, -2.24593013e-01],\n       [ 2.33243226e-01,  2.24177790e+00, -6.45415322e-01],\n       [ 1.37283813e+00,  1.89376443e+00, -6.91547078e-01],\n       [-3.56430880e-01, -1.98459448e-01, -4.72894658e-02],\n       [-9.59356091e-01,  8.41437963e-01, -5.27617975e-02],\n       [-8.73223918e-01,  3.94041511e+00,  5.03650669e+00],\n       [-9.79232746e-01,  4.64423364e-01, -4.60997743e-01],\n       [ 1.09456496e+00, -1.61172290e-01, -6.86840873e-01],\n       [ 1.56497913e+00, -4.96756713e-01, -6.84323600e-01],\n       [-5.28695226e-01,  9.98872631e-01, -2.13101117e-01],\n       [-4.75690812e-01,  7.66863646e-01, -3.20906051e-01],\n       [ 2.38654755e+00, -4.88470678e-01, -6.70697494e-01],\n       [-1.05211382e+00,  4.43757722e+00,  4.45220388e-01],\n       [ 1.34633592e+00,  5.92856908e-01, -6.43773622e-01],\n       [-9.12977228e-01,  6.34287084e-01, -2.71655066e-01],\n       [ 3.39252055e-01,  1.41317439e+00, -5.52932916e-01],\n       [-1.11174378e+00,  1.30959895e+00, -6.20242596e-01],\n       [-2.04043190e-01,  4.06421118e-01, -5.64424813e-01],\n       [-7.80466193e-01,  6.55002172e-01, -3.43889845e-01],\n       [-2.43796500e-01, -1.61172290e-01, -5.54574615e-01],\n       [ 1.73061792e+00, -2.85957275e-02, -6.86512533e-01],\n       [-3.56430880e-01, -1.93993960e+00, -6.55374966e-01],\n       [ 4.32009779e-01,  5.72141821e-01, -4.25427587e-01],\n       [-2.23919845e-01, -4.34611450e-01, -6.77045399e-01],\n       [-7.67215090e-01,  6.92289330e-01,  2.04314125e+00],\n       [-5.15444123e-01, -7.82624926e-01,  1.13473418e+00],\n       [ 3.00934941e+00,  9.15517821e-02, -6.90397889e-01],\n       [ 1.91613337e+00, -1.22178479e+00, -5.81936274e-01],\n       [-6.61206261e-01, -7.61909838e-01,  4.09526564e+00],\n       [-3.17788441e-02, -2.35746606e-01,  3.46718417e-01],\n       [ 5.97648573e-01, -1.13892444e+00, -6.52529353e-01],\n       [-2.17294293e-01,  1.29716990e+00, -2.67277201e-01],\n       [-1.44413224e-01,  1.90984204e-01, -5.32685289e-01],\n       [-2.70298707e-01, -9.56631664e-01, -4.34730551e-01],\n       [ 1.40485502e-01, -4.26325414e-01, -5.92880937e-01],\n       [-1.01898606e+00, -1.98459448e-01, -1.99278072e-02],\n       [-1.03223716e+00, -3.93181274e-01,  5.21833032e-01],\n       [-5.81699640e-01, -9.56631664e-01,  3.13760759e+00],\n       [-8.99726125e-01, -3.35179028e-01, -2.59068703e-01],\n       [-9.12977228e-01, -1.06849314e+00, -1.23902110e-01],\n       [ 1.03493499e+00, -6.99764574e-01, -6.78632375e-01],\n       [ 9.22300609e-01,  2.57272485e-01, -5.20646159e-01],\n       [ 7.97446649e-03, -5.75474047e-01,  3.46718417e-01],\n       [ 1.39934034e+00, -2.73033764e-01, -6.54718286e-01],\n       [-1.02561161e+00,  4.18355712e-02, -4.13388457e-01],\n       [-5.15444123e-01,  2.53178913e+00, -1.18429778e-01],\n       [ 1.49209806e+00, -5.13328784e-01, -6.87606999e-01],\n       [-1.19125040e+00,  5.26618073e+00,  1.84066497e+00],\n       [-1.00573495e+00,  1.28059783e+00,  1.98965461e-01],\n       [-9.12977228e-01,  6.63288207e-01,  5.71084018e-01],\n       [ 8.56045092e-01,  1.42146042e+00, -6.38848524e-01],\n       [-2.37170949e-01, -8.07483031e-01, -3.11055854e-01],\n       [-1.13824599e+00, -2.85957275e-02,  4.99943705e-01],\n       [-1.04548826e+00, -8.32341137e-01,  9.70564233e-01],\n       [-4.95567467e-01, -8.32341137e-01, -5.55669082e-01],\n       [-5.81699640e-01,  4.22993188e-01, -3.68515337e-01],\n       [ 1.28008040e+00, -1.23007082e+00, -6.28451094e-01],\n       [-2.83549811e-01, -3.51751098e-01, -2.55238071e-01],\n       [-6.41329606e-01, -2.56461694e-01,  2.14164322e+00],\n       [-9.46104987e-01,  2.65558520e-01,  3.37291785e+00],\n       [ 3.72379813e-01,  4.85138452e-01, -6.69055795e-01],\n       [ 1.64448575e+00, -7.37051732e-01, -6.71025834e-01],\n       [-9.26228332e-01,  5.76284838e-01, -4.31447152e-01],\n       [ 2.17452989e+00, -7.90910961e-01, -5.12437661e-01],\n       [ 1.27345485e+00,  4.31279223e-01, -6.82736624e-01],\n       [ 6.37401884e-01,  5.55569750e-01, -5.15173827e-01],\n       [-5.35320778e-01,  3.48418872e-01, -4.82887070e-01],\n       [-5.28695226e-01, -8.86200365e-01, -1.23902110e-01],\n       [-7.81577064e-02, -9.90270262e-02, -4.66470075e-01],\n       [ 2.12152548e+00, -7.57766820e-01, -6.76881229e-01],\n       [-9.99109401e-01,  1.74412134e-01, -5.46913351e-01],\n       [-7.14210675e-01,  6.92289330e-01,  1.20587450e+00],\n       [-6.81082917e-01, -6.66620434e-01,  1.41929543e+00],\n       [-6.74457365e-01, -1.28807307e+00,  1.93916695e+00],\n       [-5.75074088e-01, -8.90343383e-01, -5.82341292e-02],\n       [-4.02809743e-01, -7.61909838e-01, -6.33923425e-01],\n       [ 3.65754262e-01,  2.40700415e-01, -5.46913351e-01],\n       [-3.16677570e-01, -1.21349875e+00,  2.93231782e-02],\n       [-6.61206261e-01,  1.38003025e+00, -6.37754057e-01],\n       [ 1.14094382e+00, -5.17471801e-01, -6.37754057e-01],\n       [ 1.62460910e+00, -6.62477416e-01, -6.29545560e-01]])\n\n\nNow, let’s apply DBSCAN clustering to find the clusters and anomalies.\n\ndb = DBSCAN(eps=0.42, min_samples=7).fit(x_data)\n\nanomaly_count = list(db.labels_).count(-1)\ncluster_count = len(set(db.labels_)) - (1 if anomaly_count &gt; 0 else 0)\n\nprint(\"Number of clusters = %d\" % cluster_count)\nprint(\"Number of anomalies = %d\" % anomaly_count)\n\nEstimated number of clusters: 3\nEstimated number of noise points: 71\n\n\nThis function is used to plot the DBSCAN results on a scatterplot [from: handson-ml3 unsupervised learning].\n\n## Taken from handson-ml3 unsupervised learning\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=50)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\nNow, let’s use the function above to plot our DBSCAN results.\n\nplot_dbscan(db, x_data, size=50)\n\n\n\n\nWe can see the anomalies shown in the plot (marked with an X).\nNow, let’s print the anomalies in a table format.\n\nprint(\"Anomalies:\")\n\ndataset[db.labels_ == -1]\n\nAnomalies:\n\n\n\n\n\n\n\n\n\ncountry\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\n7\nAustralia\n4.8\n19.8\n8.73\n20.9\n41400\n1.160\n82.0\n1.93\n51900\n\n\n8\nAustria\n4.3\n51.3\n11.00\n47.8\n43200\n0.873\n80.5\n1.44\n46900\n\n\n10\nBahamas\n13.8\n35.0\n7.89\n43.7\n22900\n-0.393\n73.8\n1.86\n28000\n\n\n11\nBahrain\n8.6\n69.5\n4.97\n50.9\n41100\n7.440\n76.0\n2.16\n20700\n\n\n13\nBarbados\n14.2\n39.5\n7.97\n48.7\n15300\n0.321\n76.7\n1.78\n16000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n155\nUganda\n81.0\n17.1\n9.01\n28.6\n1540\n10.600\n56.8\n6.15\n595\n\n\n157\nUnited Arab Emirates\n8.6\n77.7\n3.66\n63.6\n57600\n12.500\n76.5\n1.87\n35000\n\n\n159\nUnited States\n7.3\n12.4\n17.90\n15.8\n49400\n1.220\n78.7\n1.93\n48400\n\n\n162\nVanuatu\n29.2\n46.6\n5.25\n52.7\n2950\n2.620\n63.0\n3.50\n2970\n\n\n164\nVietnam\n23.3\n72.0\n6.84\n80.2\n4490\n12.100\n73.1\n1.95\n1310\n\n\n\n\n71 rows × 10 columns"
  },
  {
    "objectID": "posts/5-anomaly-outlier-detection/index.html#resources",
    "href": "posts/5-anomaly-outlier-detection/index.html#resources",
    "title": "Anomaly/Outlier Detection: Country Data",
    "section": "Resources",
    "text": "Resources\nhttps://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data/data\nhttps://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\nhttps://www.kaggle.com/code/aditiani/kmeans-clustering-and-dbscan-for-country-data#Define-Clustering-Target\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\nhttps://www.kaggle.com/code/tanmay111999/clustering-pca-k-means-dbscan-hierarchical#Modeling\nhttps://www.kaggle.com/code/chandrimad31/clustering-with-pca-kmeans-hierarchical-dbscan#(4)-DBSCAN-Clustering\nhttps://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html\nhttps://commons.wikimedia.org/wiki/File:Physical_Map_of_the_World_(2021).svg"
  },
  {
    "objectID": "posts/3-linear-nonlinear-regression/index.html",
    "href": "posts/3-linear-nonlinear-regression/index.html",
    "title": "Linear and Nonlinear Regression: Laptop Price Prediction",
    "section": "",
    "text": "For this blog post, I will be demonstrating both linear and nonlinear regression with a dataset for laptop price prediction. Regression is a type of supervised learning, where the goal is the predict a numeric value. I will be using sklearn’s LinearRegression as well as polynomial regression and Random Forest regression to demonstrate nonlinear regression."
  },
  {
    "objectID": "posts/3-linear-nonlinear-regression/index.html#preparing-the-data",
    "href": "posts/3-linear-nonlinear-regression/index.html#preparing-the-data",
    "title": "Linear and Nonlinear Regression: Laptop Price Prediction",
    "section": "Preparing the data",
    "text": "Preparing the data\nWe will first read in the dataset that’s in a csv file. It contains various specifications for each laptop that’s listed, such as screen resolution and memory.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv(\"laptop_price.csv\", encoding='latin-1')\ndataset\n\n\n\n\n\n\n\n\nlaptop_ID\nCompany\nProduct\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice_euros\n\n\n\n\n0\n1\nApple\nMacBook Pro\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 2.3GHz\n8GB\n128GB SSD\nIntel Iris Plus Graphics 640\nmacOS\n1.37kg\n1339.69\n\n\n1\n2\nApple\nMacbook Air\nUltrabook\n13.3\n1440x900\nIntel Core i5 1.8GHz\n8GB\n128GB Flash Storage\nIntel HD Graphics 6000\nmacOS\n1.34kg\n898.94\n\n\n2\n3\nHP\n250 G6\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nNo OS\n1.86kg\n575.00\n\n\n3\n4\nApple\nMacBook Pro\nUltrabook\n15.4\nIPS Panel Retina Display 2880x1800\nIntel Core i7 2.7GHz\n16GB\n512GB SSD\nAMD Radeon Pro 455\nmacOS\n1.83kg\n2537.45\n\n\n4\n5\nApple\nMacBook Pro\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 3.1GHz\n8GB\n256GB SSD\nIntel Iris Plus Graphics 650\nmacOS\n1.37kg\n1803.60\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1298\n1316\nLenovo\nYoga 500-14ISK\n2 in 1 Convertible\n14.0\nIPS Panel Full HD / Touchscreen 1920x1080\nIntel Core i7 6500U 2.5GHz\n4GB\n128GB SSD\nIntel HD Graphics 520\nWindows 10\n1.8kg\n638.00\n\n\n1299\n1317\nLenovo\nYoga 900-13ISK\n2 in 1 Convertible\n13.3\nIPS Panel Quad HD+ / Touchscreen 3200x1800\nIntel Core i7 6500U 2.5GHz\n16GB\n512GB SSD\nIntel HD Graphics 520\nWindows 10\n1.3kg\n1499.00\n\n\n1300\n1318\nLenovo\nIdeaPad 100S-14IBR\nNotebook\n14.0\n1366x768\nIntel Celeron Dual Core N3050 1.6GHz\n2GB\n64GB Flash Storage\nIntel HD Graphics\nWindows 10\n1.5kg\n229.00\n\n\n1301\n1319\nHP\n15-AC110nv (i7-6500U/6GB/1TB/Radeon\nNotebook\n15.6\n1366x768\nIntel Core i7 6500U 2.5GHz\n6GB\n1TB HDD\nAMD Radeon R5 M330\nWindows 10\n2.19kg\n764.00\n\n\n1302\n1320\nAsus\nX553SA-XX031T (N3050/4GB/500GB/W10)\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3050 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics\nWindows 10\n2.2kg\n369.00\n\n\n\n\n1303 rows × 13 columns\n\n\n\nThese are the original columns in the dataset:\n\ndataset.columns\n\nIndex(['laptop_ID', 'Company', 'Product', 'TypeName', 'Inches',\n       'ScreenResolution', 'Cpu', 'Ram', 'Memory', 'Gpu', 'OpSys', 'Weight',\n       'Price_euros'],\n      dtype='object')\n\n\nHere is more information on the dataset, including data types of the columns:\n\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1303 entries, 0 to 1302\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   laptop_ID         1303 non-null   int64  \n 1   Company           1303 non-null   object \n 2   Product           1303 non-null   object \n 3   TypeName          1303 non-null   object \n 4   Inches            1303 non-null   float64\n 5   ScreenResolution  1303 non-null   object \n 6   Cpu               1303 non-null   object \n 7   Ram               1303 non-null   object \n 8   Memory            1303 non-null   object \n 9   Gpu               1303 non-null   object \n 10  OpSys             1303 non-null   object \n 11  Weight            1303 non-null   object \n 12  Price_euros       1303 non-null   float64\ndtypes: float64(2), int64(1), object(10)\nmemory usage: 132.5+ KB\n\n\nSince laptop_ID and Product won’t be useful in our regression (since there’s too many unique values), let’s drop them from the dataframe.\n\ndataset = dataset.drop(columns=['laptop_ID', \"Product\"])\n\nThe Ram and Weight column values have the unit in them, let’s remove them and update the datatypes:\n\ndataset[\"Ram\"] = dataset[\"Ram\"].str.replace(\"GB\", \"\").astype(\"int64\")\ndataset[\"Weight\"] = dataset[\"Weight\"].str.replace(\"kg\", \"\").astype(float)\n\nNow, we can see the updated summary of our dataframe\n\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1303 entries, 0 to 1302\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1303 non-null   object \n 1   TypeName          1303 non-null   object \n 2   Inches            1303 non-null   float64\n 3   ScreenResolution  1303 non-null   object \n 4   Cpu               1303 non-null   object \n 5   Ram               1303 non-null   int64  \n 6   Memory            1303 non-null   object \n 7   Gpu               1303 non-null   object \n 8   OpSys             1303 non-null   object \n 9   Weight            1303 non-null   float64\n 10  Price_euros       1303 non-null   float64\ndtypes: float64(3), int64(1), object(7)\nmemory usage: 112.1+ KB\n\n\nNext, for the rest of the non-numerical columns (that are more categorical), let’s encode them with LabelEncoder.\n\nfrom sklearn.preprocessing import LabelEncoder\n\ncategorical_cols = ['Company', \"TypeName\", \"ScreenResolution\", \"Cpu\", \"Memory\", \"Gpu\", \"OpSys\"]\n\nfor col in categorical_cols:\n    dataset[col] = LabelEncoder().fit_transform(dataset[col])\n\nThis is what our current dataframe looks like after the preprocessing:\n\ndataset\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice_euros\n\n\n\n\n0\n1\n4\n13.3\n23\n65\n8\n4\n58\n8\n1.37\n1339.69\n\n\n1\n1\n4\n13.3\n1\n63\n8\n2\n51\n8\n1.34\n898.94\n\n\n2\n7\n3\n15.6\n8\n74\n8\n16\n53\n4\n1.86\n575.00\n\n\n3\n1\n4\n15.4\n25\n85\n16\n29\n9\n8\n1.83\n2537.45\n\n\n4\n1\n4\n13.3\n23\n67\n8\n16\n59\n8\n1.37\n1803.60\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1298\n10\n0\n14.0\n13\n89\n4\n4\n47\n5\n1.80\n638.00\n\n\n1299\n10\n0\n13.3\n19\n89\n16\n29\n47\n5\n1.30\n1499.00\n\n\n1300\n10\n3\n14.0\n0\n34\n2\n35\n40\n5\n1.50\n229.00\n\n\n1301\n7\n3\n15.6\n0\n89\n6\n10\n21\n5\n2.19\n764.00\n\n\n1302\n2\n3\n15.6\n0\n34\n4\n26\n40\n5\n2.20\n369.00\n\n\n\n\n1303 rows × 11 columns\n\n\n\n\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1303 entries, 0 to 1302\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1303 non-null   int32  \n 1   TypeName          1303 non-null   int32  \n 2   Inches            1303 non-null   float64\n 3   ScreenResolution  1303 non-null   int32  \n 4   Cpu               1303 non-null   int32  \n 5   Ram               1303 non-null   int64  \n 6   Memory            1303 non-null   int32  \n 7   Gpu               1303 non-null   int32  \n 8   OpSys             1303 non-null   int32  \n 9   Weight            1303 non-null   float64\n 10  Price_euros       1303 non-null   float64\ndtypes: float64(3), int32(7), int64(1)\nmemory usage: 76.5 KB"
  },
  {
    "objectID": "posts/3-linear-nonlinear-regression/index.html#linear-regression",
    "href": "posts/3-linear-nonlinear-regression/index.html#linear-regression",
    "title": "Linear and Nonlinear Regression: Laptop Price Prediction",
    "section": "Linear Regression",
    "text": "Linear Regression\nWith our preprocessed dataframe, let’s separate them into the x and y values by separating out the Price_euros column for y.\n\nx_values = dataset.drop(['Price_euros'], axis=1).values\ny_values = dataset['Price_euros'].values\n\nNow, let’s split the dataset into a training set and a testing set using train_test_split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test= train_test_split(x_values,y_values, shuffle=True, test_size=.3, random_state=42)\n\nScale data to make sure that columns are standarized using RobustScaler\n\nfrom sklearn.preprocessing import RobustScaler\n\nrobust_scaler = RobustScaler()\n\nx_train = robust_scaler.fit_transform(x_train)\nx_test = robust_scaler.fit_transform(x_test)\n\nWe can now import LinearRegression from sklearn and use it to fit the training data\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(x_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nUsing the score method, we can get the coefficient of determination\n\nlin_reg.score(x_train, y_train)\n\n0.6761254164098996\n\n\n\nlin_reg.score(x_test, y_test)\n\n0.6473397524255087\n\n\n\nScatterplot of predictions\n\ny_pred = lin_reg.predict(x_test)\nplt.scatter(y_pred, y_test)\nplt.plot(range(0, 5000), range(0,5000), c='red')"
  },
  {
    "objectID": "posts/3-linear-nonlinear-regression/index.html#nonlinear-regression",
    "href": "posts/3-linear-nonlinear-regression/index.html#nonlinear-regression",
    "title": "Linear and Nonlinear Regression: Laptop Price Prediction",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\nNow, let’s apply two different nonlinear regressions to the dataset.\n\nPolynomial Regression\nStarting off with polynomial regression, we can use PolynomialFeatures from sklearn to preprocess the data to then pass into a LinearRegression.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_features = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = polynomial_features.fit_transform(x_train)\n\nlin_reg_poly = LinearRegression()\nlin_reg_poly.fit(x_train_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nDisplay the coefficient of determination with using the training and testing data.\n\nlin_reg_poly.score(x_train_poly, y_train)\n\n0.7935136564934857\n\n\n\nx_test_poly = polynomial_features.fit_transform(x_test)\nlin_reg_poly.score(x_test_poly, y_test)\n\n0.6892366284789773\n\n\n\nScatterplot\nLet’s display a scatter plot with the predictions using polynomial regression\n\ny_pred = lin_reg_poly.predict(x_test_poly)\n\nplt.scatter(y_pred, y_test)\nplt.plot(range(0, 5000), range(0,5000), c='red')\n\n\n\n\n\n\n\nRandomForestRegressor\nNow, let’s try RandomForestRegressor to perform nonlinear regression, which uses decision trees.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrandom_forest = RandomForestRegressor()\nrandom_forest.fit(x_train, y_train)\nrandom_forest.score(x_test, y_test)\n\n0.8238353733914368\n\n\n\nScatterplot\n\ny_pred = random_forest.predict(x_test)\n\nplt.scatter(y_pred, y_test)\nplt.plot(range(0, 5000), range(0,5000), c='red')"
  },
  {
    "objectID": "posts/3-linear-nonlinear-regression/index.html#resources",
    "href": "posts/3-linear-nonlinear-regression/index.html#resources",
    "title": "Linear and Nonlinear Regression: Laptop Price Prediction",
    "section": "Resources",
    "text": "Resources\nhttps://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\nhttps://www.youtube.com/watch?v=wH_ezgftiy0\nhttps://www.kaggle.com/datasets/muhammetvarl/laptop-price/data\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\nhttps://learning.oreilly.com/library/view/hands-on-machine-learning/9781098125967/\nhttps://www.kaggle.com/code/ahmadmostafade/laptop-price-prediction\nhttps://www.kaggle.com/code/mohamedanwar11/laptop-price-prediction-linear-regression/notebook#data-scaling\nhttps://commons.wikimedia.org/wiki/Laptop [Picture]"
  },
  {
    "objectID": "posts/1-prob-theory-rand-var/index.html",
    "href": "posts/1-prob-theory-rand-var/index.html",
    "title": "Probability Theory & Random Variables: Sentiment Analysis",
    "section": "",
    "text": "In this blog post, we will use Naive Bayes to demonstrate probability theory & random variables, as well as apply it to a sentiment analysis dataset. Naive Bayes uses Bayes’ rule to calculate probabilities and to predict classifications. There are multiple different types of Naive Bayes algorithms and classifiers–in this post we will be using Gaussian Naive Bayes and Multinomial Naive Bayes."
  },
  {
    "objectID": "posts/1-prob-theory-rand-var/index.html#preparing-the-data",
    "href": "posts/1-prob-theory-rand-var/index.html#preparing-the-data",
    "title": "Probability Theory & Random Variables: Sentiment Analysis",
    "section": "Preparing the data",
    "text": "Preparing the data\nFirst, let’s read in our dataset from the csv file:\n\nimport pandas as pd\n\ndataset = pd.read_csv(\"Reddit_Data.csv\")\n\nHere’s a preview of the contents of the current dataset\n\ndataset\n\n\n\n\n\n\n\n\nclean_comment\ncategory\n\n\n\n\n0\nfamily mormon have never tried explain them t...\n1\n\n\n1\nbuddhism has very much lot compatible with chr...\n1\n\n\n2\nseriously don say thing first all they won get...\n-1\n\n\n3\nwhat you have learned yours and only yours wha...\n0\n\n\n4\nfor your own benefit you may want read living ...\n1\n\n\n...\n...\n...\n\n\n37244\njesus\n0\n\n\n37245\nkya bhai pure saal chutiya banaya modi aur jab...\n1\n\n\n37246\ndownvote karna tha par upvote hogaya\n0\n\n\n37247\nhaha nice\n1\n\n\n37248\nfacebook itself now working bjp’ cell\n0\n\n\n\n\n37249 rows × 2 columns\n\n\n\nIt contains two columns, clean_comment and category [1 for positive, 0 for neutral, -1 for negative sentiment]\n\ndataset.columns\n\nIndex(['clean_comment', 'category'], dtype='object')\n\n\nHere is information on the dataframe including the datatypes\n\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 37249 entries, 0 to 37248\nData columns (total 2 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   clean_comment  37149 non-null  object\n 1   category       37249 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 582.1+ KB\n\n\nLet’s check if there are null values\n\ndataset.isnull().sum()\n\nclean_comment    100\ncategory           0\ndtype: int64\n\n\nSince there are null values, let’s remove them from the dataset\n\ndataset = dataset.dropna()\n\nHere is our updated dataset\n\ndataset\n\n\n\n\n\n\n\n\nclean_comment\ncategory\n\n\n\n\n0\nfamily mormon have never tried explain them t...\n1\n\n\n1\nbuddhism has very much lot compatible with chr...\n1\n\n\n2\nseriously don say thing first all they won get...\n-1\n\n\n3\nwhat you have learned yours and only yours wha...\n0\n\n\n4\nfor your own benefit you may want read living ...\n1\n\n\n...\n...\n...\n\n\n37244\njesus\n0\n\n\n37245\nkya bhai pure saal chutiya banaya modi aur jab...\n1\n\n\n37246\ndownvote karna tha par upvote hogaya\n0\n\n\n37247\nhaha nice\n1\n\n\n37248\nfacebook itself now working bjp’ cell\n0\n\n\n\n\n37149 rows × 2 columns\n\n\n\n\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 37149 entries, 0 to 37248\nData columns (total 2 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   clean_comment  37149 non-null  object\n 1   category       37149 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 870.7+ KB"
  },
  {
    "objectID": "posts/1-prob-theory-rand-var/index.html#applying-naive-bayes",
    "href": "posts/1-prob-theory-rand-var/index.html#applying-naive-bayes",
    "title": "Probability Theory & Random Variables: Sentiment Analysis",
    "section": "Applying Naive Bayes",
    "text": "Applying Naive Bayes\nFirst, let’s separate the x and y columns (clean comments are our x values and the sentiments/categories are our y values):\n\nfrom sklearn.model_selection import train_test_split\n\nx_values = dataset['clean_comment']\ny_values = dataset['category']\n\nThen, we separate the dataset into training and testing sets using train_test_split\n\nx_train, x_test, y_train, y_test= train_test_split(x_values,y_values, shuffle=True, test_size=.2, random_state=42)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n\n(29719,)\n(29719,)\n(7430,)\n(7430,)\n\n\nNow, let’s import and create CountVectorizer and TfidfTransformer instances to tokenize our text values\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nvectorizer = CountVectorizer(analyzer=\"word\")\ntfidf_transformer = TfidfTransformer()\n\nWe can apply the fit_transform/transform functions to the training/testing x values\n\nimport numpy as np\n\nx_train = vectorizer.fit_transform(x_train)\nx_train = tfidf_transformer.fit_transform(X=x_train)\nx_test = vectorizer.transform(x_test)\nx_test = tfidf_transformer.transform(X=x_test)\n\nNow, let’s apply Gaussian Naive Bayes to the dataset. We can use GaussianNB from sklearn to classify our data.\n\nfrom sklearn.naive_bayes import GaussianNB\n\nx_train = x_train.toarray()\nx_test = x_test.toarray()\n\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nWe can use the GaussianNB classifier to predict the y values from our test x values:\n\ny_pred = gnb.predict(x_test)\n\nUsing the sklearn metrics accuracy_score function, let’s print our accuracy score\n\nfrom sklearn.metrics import accuracy_score\n\nprint('Accuracy score: ', (accuracy_score(y_test, y_pred)))\n\nAccuracy score:  0.5281292059219381\n\n\nNow, let’s try applying Multinomial Naive Bayes, which is commonly used in text classification\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nmultinomialNB = MultinomialNB(force_alpha=True)\nmultinomialNB.fit(x_train, y_train)\n\nMultinomialNB(force_alpha=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB(force_alpha=True)\n\n\nAfter fitting the classifier to the training data, let’s use it to predict the y values of our test data\n\ny_pred_multi_NB = multinomialNB.predict(x_test)\n\nPrint the accuracy score:\n\nprint('Accuracy score: ', (accuracy_score(y_test, y_pred_multi_NB)))\n\nAccuracy score:  0.5364737550471064\n\n\nTo demonstrate specific examples, let’s try passing in some strings:\n\ntest_string = [\"This is good\"]\ntest = vectorizer.transform(test_string)\ntest = tfidf_transformer.transform(X=test)\nprint(multinomialNB.predict(test))\n\ntest_string = [\"horrible terrible bad\"]\ntest = vectorizer.transform(test_string)\ntest = tfidf_transformer.transform(X=test)\nprint(multinomialNB.predict(test))\n\n[1]\n[-1]\n\n\nAs you can see, the strings were classified as positive and negative sentiment, respectively."
  },
  {
    "objectID": "posts/1-prob-theory-rand-var/index.html#data-visualization-confusion-matrix",
    "href": "posts/1-prob-theory-rand-var/index.html#data-visualization-confusion-matrix",
    "title": "Probability Theory & Random Variables: Sentiment Analysis",
    "section": "Data Visualization: Confusion Matrix",
    "text": "Data Visualization: Confusion Matrix\nLet’s use the MultinomialNB classifier to make predictions to add to a confusion matrix:\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ny_train_pred = cross_val_predict(multinomialNB, x_train, y_train)\nplt.rc('font', size=10)\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\nplt.show()"
  },
  {
    "objectID": "posts/1-prob-theory-rand-var/index.html#resources",
    "href": "posts/1-prob-theory-rand-var/index.html#resources",
    "title": "Probability Theory & Random Variables: Sentiment Analysis",
    "section": "Resources",
    "text": "Resources\nhttps://commons.wikimedia.org/wiki/File:Reddit_logo.svg\nhttps://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset/data\nhttps://scikit-learn.org/stable/modules/naive_bayes.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\nhttps://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python#3.-Types-of-Naive-Bayes-algorithm-\nhttps://github.com/maptv/handson-ml3/blob/main/03_classification.ipynb\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my blog for my CS 5805 Machine Learning final project!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805: Blog Project - Julia Chen",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nProbability Theory & Random Variables: Sentiment Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJulia Chen\n\n\n\n\n\n\n  \n\n\n\n\nClustering: Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJulia Chen\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression: Laptop Price Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJulia Chen\n\n\n\n\n\n\n  \n\n\n\n\nClassification: Fruit Recognition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJulia Chen\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Detection: Country Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJulia Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering/index.html",
    "href": "posts/2-clustering/index.html",
    "title": "Clustering: Penguins",
    "section": "",
    "text": "For this blog post, I will be applying the concept of clustering to a dataset containing information about penguins (without data on species or their classification). Clustering is an unsupervised machine learning task that groups objects/instances together without having labels/classification. In this post, I will be using the clustering algorithm DBSCAN, which is short for “density-based spatial clustering of applications with noise.” There are many other clustering algorithms other than DBSCAN, such as k-means and spectral clustering.\n\n\nFirst, let’s read in our dataset from the csv file.\n\nimport pandas as pd\n\ndataset = pd.read_csv(\"penguins.csv\")\ndataset\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n...\n...\n...\n...\n...\n...\n\n\n339\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nMALE\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nMALE\n\n\n\n\n344 rows × 5 columns\n\n\n\nSince there’s rows with NaN values, let’s drop them:\n\ndataset = dataset.dropna()\n\n\ndataset[\"sex\"].value_counts()\n\nsex\nMALE      169\nFEMALE    165\n.           1\nName: count, dtype: int64\n\n\nThere is one penguin with ‘sex’ value just marked with ‘.’ Let’s remove it from the dataset\n\ndataset = dataset[dataset['sex'] != \".\"]\n\n\ndataset\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n5\n39.3\n20.6\n190.0\n3650.0\nMALE\n\n\n...\n...\n...\n...\n...\n...\n\n\n338\n47.2\n13.7\n214.0\n4925.0\nFEMALE\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nMALE\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nMALE\n\n\n\n\n334 rows × 5 columns\n\n\n\n\ndataset.describe()\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n334.000000\n334.000000\n334.000000\n334.000000\n\n\nmean\n43.986826\n17.173952\n214.347305\n4207.185629\n\n\nstd\n5.461540\n1.973277\n263.648447\n804.009294\n\n\nmin\n32.100000\n13.100000\n-132.000000\n2700.000000\n\n\n25%\n39.500000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.575000\n18.700000\n213.000000\n4768.750000\n\n\nmax\n59.600000\n21.500000\n5000.000000\n6300.000000\n\n\n\n\n\n\n\nNow, let’s separate the male and female penguins to form separate clusters for each.\n\nimport matplotlib.pyplot as plt\n\ndataset_male = dataset[dataset[\"sex\"] == \"MALE\"]\ndataset_female = dataset[dataset[\"sex\"] == \"FEMALE\"]\n\nAnd make sure they don’t have null values:\n\ndataset_male.isnull().sum()\n\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\n\ndataset_female.isnull().sum()\n\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\nHere’s some information about each dataset:\n\ndataset_male.describe()\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n169.000000\n169.000000\n169.000000\n169.000000\n\n\nmean\n45.831953\n17.904734\n230.928994\n4543.934911\n\n\nstd\n5.359109\n1.866267\n370.226034\n785.610576\n\n\nmin\n34.600000\n14.100000\n-132.000000\n3250.000000\n\n\n25%\n41.000000\n16.100000\n193.000000\n3900.000000\n\n\n50%\n46.800000\n18.500000\n201.000000\n4300.000000\n\n\n75%\n50.300000\n19.400000\n219.000000\n5300.000000\n\n\nmax\n59.600000\n21.500000\n5000.000000\n6300.000000\n\n\n\n\n\n\n\n\ndataset_female.describe()\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n165.000000\n165.000000\n165.000000\n165.000000\n\n\nmean\n42.096970\n16.425455\n197.363636\n3862.272727\n\n\nstd\n4.903476\n1.795681\n12.500776\n666.172050\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n37.600000\n14.500000\n187.000000\n3350.000000\n\n\n50%\n42.800000\n17.000000\n193.000000\n3650.000000\n\n\n75%\n46.200000\n17.800000\n210.000000\n4550.000000\n\n\nmax\n58.000000\n20.700000\n222.000000\n5200.000000\n\n\n\n\n\n\n\nLet’s create separate DataFrames for each attribute\n\nculmen_length_mm = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\nculmen_depth_mm = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\nflipper_length_mm = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\nbody_mass_g = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\n\nAnd display them in box plots\n\n\nculmen_length_mm[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"culmen_length_mm\")\nax = culmen_depth_mm[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"culmen_depth_mm\")\nax = flipper_length_mm[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"flipper_length_mm\")\nax = body_mass_g[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"body_mass_g\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s apply DBSCAN to our penguin datasets. Let’s first scale the x values for each dataset.\n\nfrom sklearn.preprocessing import StandardScaler\n\nx_male_clusters = StandardScaler().fit_transform(dataset_male[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm','body_mass_g']])\nx_female_clusters = StandardScaler().fit_transform(dataset_female[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm','body_mass_g']])\n\nWe can then use DBSCAN to form clusters using the scaled datasets\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\n\ndbscan_male = DBSCAN(eps=0.5, min_samples=4).fit(x_male_clusters)\ndbscan_female = DBSCAN(eps=0.5, min_samples=4).fit(x_female_clusters)\n\nUsing the results of the DBSCAN clustering, we can get the number of clusters and anomalies\n\nlabels_male = dbscan_male.labels_\nlabels_female = dbscan_female.labels_\n\nanomalies_male = list(labels_male).count(-1)\nanomalies_female = list(labels_female).count(-1)\n\nclusters_male = len(set(labels_male)) - (1 if anomalies_male &gt; 0 else 0)\nclusters_female = len(set(labels_female)) - (1 if anomalies_female &gt; 0 else 0)\n\nprint(\"---Male Penguins---\")\nprint(\"Number of clusters = %d\" % clusters_male)\nprint(\"Number of anomalies = %d\" % anomalies_male)\n\nprint(\"\\n---Female Penguins---\")\nprint(\"Number of clusters = %d\" % clusters_female)\nprint(\"Number of anomalies = %d\" % anomalies_female)\n\n---Male Penguins---\nNumber of clusters = 3\nNumber of anomalies = 24\n\n---Female Penguins---\nNumber of clusters = 3\nNumber of anomalies = 51\n\n\nThis function is used to plot the DBSCAN results on a scatterplot [from: handson-ml3 unsupervised learning].\n\n## Taken from handson-ml3 unsupervised learning\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\nUsing the function above, plot the DBSCAN results for the male penguins and the female penguins.\n\nplot_dbscan(dbscan_male, x_male_clusters, size=100)\n\n\n\n\n\nplot_dbscan(dbscan_female, x_female_clusters, size=100)\n\n\n\n\n\n\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\nhttps://www.kaggle.com/datasets/youssefaboelwafa/clustering-penguins-species/data\nhttps://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\nhttps://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html\nhttps://www.tutorialspoint.com/plot-multiple-boxplots-in-one-graph-in-pandas-or-matplotlib\nhttps://www.kaggle.com/code/kesyafebriana/clustering-penguins-species-using-dbscan\nhttps://commons.wikimedia.org/wiki/File:Adelie_Penguins_on_iceberg.jpg"
  },
  {
    "objectID": "posts/2-clustering/index.html#prepare-the-data",
    "href": "posts/2-clustering/index.html#prepare-the-data",
    "title": "Clustering: Penguins",
    "section": "",
    "text": "First, let’s read in our dataset from the csv file.\n\nimport pandas as pd\n\ndataset = pd.read_csv(\"penguins.csv\")\ndataset\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n...\n...\n...\n...\n...\n...\n\n\n339\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nMALE\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nMALE\n\n\n\n\n344 rows × 5 columns\n\n\n\nSince there’s rows with NaN values, let’s drop them:\n\ndataset = dataset.dropna()\n\n\ndataset[\"sex\"].value_counts()\n\nsex\nMALE      169\nFEMALE    165\n.           1\nName: count, dtype: int64\n\n\nThere is one penguin with ‘sex’ value just marked with ‘.’ Let’s remove it from the dataset\n\ndataset = dataset[dataset['sex'] != \".\"]\n\n\ndataset\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n5\n39.3\n20.6\n190.0\n3650.0\nMALE\n\n\n...\n...\n...\n...\n...\n...\n\n\n338\n47.2\n13.7\n214.0\n4925.0\nFEMALE\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nMALE\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nMALE\n\n\n\n\n334 rows × 5 columns\n\n\n\n\ndataset.describe()\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n334.000000\n334.000000\n334.000000\n334.000000\n\n\nmean\n43.986826\n17.173952\n214.347305\n4207.185629\n\n\nstd\n5.461540\n1.973277\n263.648447\n804.009294\n\n\nmin\n32.100000\n13.100000\n-132.000000\n2700.000000\n\n\n25%\n39.500000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.575000\n18.700000\n213.000000\n4768.750000\n\n\nmax\n59.600000\n21.500000\n5000.000000\n6300.000000\n\n\n\n\n\n\n\nNow, let’s separate the male and female penguins to form separate clusters for each.\n\nimport matplotlib.pyplot as plt\n\ndataset_male = dataset[dataset[\"sex\"] == \"MALE\"]\ndataset_female = dataset[dataset[\"sex\"] == \"FEMALE\"]\n\nAnd make sure they don’t have null values:\n\ndataset_male.isnull().sum()\n\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\n\ndataset_female.isnull().sum()\n\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\nHere’s some information about each dataset:\n\ndataset_male.describe()\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n169.000000\n169.000000\n169.000000\n169.000000\n\n\nmean\n45.831953\n17.904734\n230.928994\n4543.934911\n\n\nstd\n5.359109\n1.866267\n370.226034\n785.610576\n\n\nmin\n34.600000\n14.100000\n-132.000000\n3250.000000\n\n\n25%\n41.000000\n16.100000\n193.000000\n3900.000000\n\n\n50%\n46.800000\n18.500000\n201.000000\n4300.000000\n\n\n75%\n50.300000\n19.400000\n219.000000\n5300.000000\n\n\nmax\n59.600000\n21.500000\n5000.000000\n6300.000000\n\n\n\n\n\n\n\n\ndataset_female.describe()\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n165.000000\n165.000000\n165.000000\n165.000000\n\n\nmean\n42.096970\n16.425455\n197.363636\n3862.272727\n\n\nstd\n4.903476\n1.795681\n12.500776\n666.172050\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n37.600000\n14.500000\n187.000000\n3350.000000\n\n\n50%\n42.800000\n17.000000\n193.000000\n3650.000000\n\n\n75%\n46.200000\n17.800000\n210.000000\n4550.000000\n\n\nmax\n58.000000\n20.700000\n222.000000\n5200.000000\n\n\n\n\n\n\n\nLet’s create separate DataFrames for each attribute\n\nculmen_length_mm = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\nculmen_depth_mm = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\nflipper_length_mm = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\nbody_mass_g = pd.DataFrame({\"Male\": dataset_male[\"culmen_length_mm\"], \"Female\": dataset_female[\"culmen_length_mm\"]})\n\nAnd display them in box plots\n\n\nculmen_length_mm[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"culmen_length_mm\")\nax = culmen_depth_mm[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"culmen_depth_mm\")\nax = flipper_length_mm[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"flipper_length_mm\")\nax = body_mass_g[[\"Male\", \"Female\"]].plot(kind=\"box\", title=\"body_mass_g\")"
  },
  {
    "objectID": "posts/2-clustering/index.html#clustering",
    "href": "posts/2-clustering/index.html#clustering",
    "title": "Clustering: Penguins",
    "section": "",
    "text": "Now, let’s apply DBSCAN to our penguin datasets. Let’s first scale the x values for each dataset.\n\nfrom sklearn.preprocessing import StandardScaler\n\nx_male_clusters = StandardScaler().fit_transform(dataset_male[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm','body_mass_g']])\nx_female_clusters = StandardScaler().fit_transform(dataset_female[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm','body_mass_g']])\n\nWe can then use DBSCAN to form clusters using the scaled datasets\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\n\ndbscan_male = DBSCAN(eps=0.5, min_samples=4).fit(x_male_clusters)\ndbscan_female = DBSCAN(eps=0.5, min_samples=4).fit(x_female_clusters)\n\nUsing the results of the DBSCAN clustering, we can get the number of clusters and anomalies\n\nlabels_male = dbscan_male.labels_\nlabels_female = dbscan_female.labels_\n\nanomalies_male = list(labels_male).count(-1)\nanomalies_female = list(labels_female).count(-1)\n\nclusters_male = len(set(labels_male)) - (1 if anomalies_male &gt; 0 else 0)\nclusters_female = len(set(labels_female)) - (1 if anomalies_female &gt; 0 else 0)\n\nprint(\"---Male Penguins---\")\nprint(\"Number of clusters = %d\" % clusters_male)\nprint(\"Number of anomalies = %d\" % anomalies_male)\n\nprint(\"\\n---Female Penguins---\")\nprint(\"Number of clusters = %d\" % clusters_female)\nprint(\"Number of anomalies = %d\" % anomalies_female)\n\n---Male Penguins---\nNumber of clusters = 3\nNumber of anomalies = 24\n\n---Female Penguins---\nNumber of clusters = 3\nNumber of anomalies = 51\n\n\nThis function is used to plot the DBSCAN results on a scatterplot [from: handson-ml3 unsupervised learning].\n\n## Taken from handson-ml3 unsupervised learning\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\nUsing the function above, plot the DBSCAN results for the male penguins and the female penguins.\n\nplot_dbscan(dbscan_male, x_male_clusters, size=100)\n\n\n\n\n\nplot_dbscan(dbscan_female, x_female_clusters, size=100)"
  },
  {
    "objectID": "posts/2-clustering/index.html#resources",
    "href": "posts/2-clustering/index.html#resources",
    "title": "Clustering: Penguins",
    "section": "",
    "text": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\nhttps://www.kaggle.com/datasets/youssefaboelwafa/clustering-penguins-species/data\nhttps://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\nhttps://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html\nhttps://www.tutorialspoint.com/plot-multiple-boxplots-in-one-graph-in-pandas-or-matplotlib\nhttps://www.kaggle.com/code/kesyafebriana/clustering-penguins-species-using-dbscan\nhttps://commons.wikimedia.org/wiki/File:Adelie_Penguins_on_iceberg.jpg"
  },
  {
    "objectID": "posts/4-classification/index.html",
    "href": "posts/4-classification/index.html",
    "title": "Classification: Fruit Recognition",
    "section": "",
    "text": "This blog post will apply the topic of classification with detecting different types of fruit (peach, banana, guava). Classification is a supervised learning task that involves predicting which class/group something (e.g. an image) belongs in. For this post, we will mainly use a grid search approach using a support vector classifer to classify the fruit images."
  },
  {
    "objectID": "posts/4-classification/index.html#processing-the-data",
    "href": "posts/4-classification/index.html#processing-the-data",
    "title": "Classification: Fruit Recognition",
    "section": "Processing the data",
    "text": "Processing the data\nFirst, making sure scikit-image is installed for image processing\n\n%pip install scikit-image\n\nRequirement already satisfied: scikit-image in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.22.0)\nRequirement already satisfied: numpy&gt;=1.22 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.26.2)\nRequirement already satisfied: scipy&gt;=1.8 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.11.4)\nRequirement already satisfied: networkx&gt;=2.8 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (3.2.1)\nRequirement already satisfied: pillow&gt;=9.0.1 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (10.1.0)\nRequirement already satisfied: imageio&gt;=2.27 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2.33.0)\nRequirement already satisfied: tifffile&gt;=2022.8.12 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2023.9.26)\nRequirement already satisfied: packaging&gt;=21 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (23.2)\nRequirement already satisfied: lazy_loader&gt;=0.3 in c:\\users\\julia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (0.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport os\nfrom skimage.io import imread\nfrom skimage import io\nfrom skimage.transform import resize\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nLet’s get the directories of the images:\n\ncurr_dir = ''\npeach_dir = os.path.join(curr_dir, 'peach_data') #0\nbanana_dir = os.path.join(curr_dir, 'banana_data') #1\nguava_dir = os.path.join(curr_dir, 'guava_data') #2\n\nHere are some example images from our dataset\n\npeach = io.imread(os.path.join(peach_dir, 'Peach001.png'))\nio.imshow(peach)\nio.show()\n\n\n\n\n\nguava = io.imread(os.path.join(guava_dir, 'Guava1.png'))\nio.imshow(guava)\nio.show()\n\n\n\n\n\nbanana = io.imread(os.path.join(banana_dir, 'Banana01.png'))\nio.imshow(banana)\nio.show()\n\n\n\n\nNow, let’s process all of the images to add to a data array and label array\n\nimage_data = []\nimage_labels = []\n\nRead in images from each directory, resize the images, and add them to data array with a corresponding value in the labels array:\n\nfor file in os.listdir(peach_dir):\n    img_path = os.path.join(peach_dir, file)\n    img = imread(img_path)\n    img = resize(img, (25, 25))\n    image_data.append(img.flatten())\n    image_labels.append(0)\n\nfor file in os.listdir(banana_dir):\n    img_path = os.path.join(banana_dir, file)\n    img = imread(img_path)\n    img = resize(img, (25, 25))\n    image_data.append(img.flatten())\n    image_labels.append(1)\n\nfor file in os.listdir(guava_dir):\n    img_path = os.path.join(guava_dir, file)\n    img = imread(img_path)\n    img = resize(img, (25, 25))\n    image_data.append(img.flatten())\n    image_labels.append(2)\n\nConvert the data and label lists to arrays\n\ndata_array = np.asarray(image_data)\nlabel_array = np.asarray(image_labels)"
  },
  {
    "objectID": "posts/4-classification/index.html#classification",
    "href": "posts/4-classification/index.html#classification",
    "title": "Classification: Fruit Recognition",
    "section": "Classification",
    "text": "Classification\nSplit the data into separate training and testing sets using train_test_split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(data_array, label_array, test_size=0.1, shuffle=True, stratify=label_array)\n\nUsing a support vector classification (SVC) and grid search to classify fruit data\n\nclassifier = SVC()\n\nparameters = [{'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'C': [0.1, 1, 10, 100, 1000]}]\n\ngrid_search = GridSearchCV(classifier, parameters)\n\ngrid_search.fit(x_train, y_train)\n\nGridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [0.1, 1, 10, 100, 1000],\n                          'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=SVC(),\n             param_grid=[{'C': [0.1, 1, 10, 100, 1000],\n                          'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}])estimator: SVCSVC()SVCSVC()\n\n\n\nTest the performance\nUse the best estimator found in the grid search to predict the y values and calculate an accuracy score\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = grid_search.best_estimator_.predict(x_test)\n\nscore = accuracy_score(y_pred, y_test)\n\nprint('Accuracy Score: ', score)\n\nAccuracy Score:  0.8833333333333333\n\n\nTrying a Random Forest Classifier:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nforest_clf = RandomForestClassifier(random_state=42)\nforest_clf.fit(x_train, y_train)\n\nprint(cross_val_score(forest_clf, x_train, y_train, cv=3, scoring=\"accuracy\"))\n\n[0.88888889 0.88268156 0.88268156]"
  },
  {
    "objectID": "posts/4-classification/index.html#confusion-matrix-data-visualization",
    "href": "posts/4-classification/index.html#confusion-matrix-data-visualization",
    "title": "Classification: Fruit Recognition",
    "section": "Confusion Matrix Data Visualization",
    "text": "Confusion Matrix Data Visualization\nA confusion matrix will show the number of times an image of fruit i is classified as a fruit of class j. We will use cross_val_predict to get a set of predictions to create our confusion matrix.\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ny_train_pred = cross_val_predict(grid_search, x_train, y_train)\nplt.rc('font', size=10)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\nplt.show()"
  },
  {
    "objectID": "posts/4-classification/index.html#resources-used",
    "href": "posts/4-classification/index.html#resources-used",
    "title": "Classification: Fruit Recognition",
    "section": "Resources Used:",
    "text": "Resources Used:\nhttps://www.kaggle.com/datasets/chrisfilo/fruit-recognition/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\nhttps://github.com/maptv/handson-ml3/blob/main/03_classification.ipynb\nhttps://github.com/computervisioneng/image-classification-python-scikit-learn/blob/master/main.py\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
  }
]